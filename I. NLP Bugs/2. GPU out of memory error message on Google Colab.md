---
tags:
  - bugs
  - gpu
  - out-of-memory
  - gg-colab
completed: false
Date: 2024-05-18
---
### 1. Description and case
##### 1.1. Description
- When running deep learning models or any computationally intensive operations on Google Colab, you might encounter a "GPU out of memory" error. 
- This error occurs when the model or the operation requires more GPU memory than is available. This is a common issue, especially with large datasets, complex models, or high batch sizes.
##### 1.2. Case
- In a recent project, while training a neural network model using TensorFlow on a relatively large dataset, the following error was encountered:
```error
RuntimeError: CUDA out of memory. Tried to allocate 1.00 GiB (GPU 0; 11.00 GiB total capacity; 7.50 GiB already allocated; 200.00 MiB free; 3.90 GiB cached)
```
##### 1.3. Reason
- Google Colab resource allocation is dynamic, based on users past usage. Suppose if a user has been using more resources recently and a new user who is less frequently uses Colab, he will be given relatively more preference in resource allocation.
### 2. Steps to Reproduce
1. Open Google Colab and ensure the GPU runtime is enabled.
2. Load the GPT-3 or mT5-large pre-trained model.
3. Prepare a large text dataset for training.
4. Start the fine-tuning process for text normalization.
5. Observe the "GPU out of memory" error message during the training.
### 3. Expected Behavior
- The model should fine-tune on the dataset without running out of memory, utilizing the available GPU resources efficiently. The training process should complete without interruptions.
### 4. Actual Behavior
- The training process fails with a "GPU out of memory" error. This interrupts the training, requiring adjustments to the model or dataset, or a restart of the kernel.
### 5. Solution or workaround
To address the GPU out of memory issue, consider the following solutions and workarounds:
##### 5.1. Reduce Batch Size:
Lowering the batch size decreases the amount of memory required per batch during training.
```python
batch_size = 8  # Reduce this value if you encounter memory issues
```
##### 5.2. Optimize Model
Simplify the model architecture or use model distillation techniques to reduce the size.
```python
from transformers import TFAutoModelForSeq2SeqLM
model = TFAutoModelForSeq2SeqLM.from_pretrained("google/mt5-large")
```
##### 5.3. Use Mixed Precision Training: 
Utilize mixed precision training to reduce memory usage by using both 16-bit and 32-bit floating point types.
```python
from tensorflow.keras.mixed_precision import experimental as mixed_precision
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_policy(policy)
```
##### 5.4. Clear Session
Clear the TensorFlow session to free up memory after each training session.
```python
import tensorflow.keras.backend as K
K.clear_session()
```
##### 5.5. Enable Memory Growth: 
Allow TensorFlow to allocate memory as needed, rather than pre-allocating all available memory.
```python
import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)
```
##### 5.6. Use Model Checkpoints
Save the model at regular intervals and restart the training from the last checkpoint if an out-of-memory error occurs.
```python
from transformers import Trainer, TrainingArguments
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    save_steps=10_000,               # save checkpoint every 10,000 steps
    save_total_limit=2,              # only keep the last 2 checkpoints
)
trainer = Trainer(
    model=model,                     # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,              # training arguments, defined above
    train_dataset=train_dataset,     # training dataset
)
trainer.train()
```
##### 5.7. Workarond
- Hence to get the max out of Colab , close all your Colab tabs and all other active sessions ,restart runtime for the one you want to use. You'll definitely get better GPU allocation.
### 6. References
##### 6.1. Relevent Resourses or Documentation
##### 6.2. Related Issues
##### 6.3. The code to show the amount of memory you have.
- Note that if you try in load images bigger than the total memory, it will fail.
```python
# memory footprint support libraries/code
!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
!pip install gputil
!pip install psutil
!pip install humanize

import psutil
import humanize
import os
import GPUtil as GPU

GPUs = GPU.getGPUs()
# XXX: only one GPU on Colab and isnâ€™t guaranteed
gpu = GPUs[0]
def printm():
    process = psutil.Process(os.getpid())
    print("Gen RAM Free: " + humanize.naturalsize(psutil.virtual_memory().available), " |     Proc size: " + humanize.naturalsize(process.memory_info().rss))
    print("GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))
printm()
```